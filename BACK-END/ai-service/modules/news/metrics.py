"""
Syst√®me de m√©triques pour le module News
- Moyenne glissante sur les 100 derni√®res op√©rations
- Compteurs de cache hits/misses
- Statistiques de performance
"""
import redis
import os
import logging
from typing import Dict, Any, Optional
from datetime import datetime, timedelta
from collections import deque
import json

logger = logging.getLogger(__name__)


class NewsMetrics:
    """Gestionnaire de m√©triques pour les actualit√©s"""

    def __init__(self):
        redis_url = os.getenv("REDIS_URL", "redis://redis:6379")
        self.redis_client = None

        try:
            self.redis_client = redis.from_url(redis_url, decode_responses=True)
            self.redis_client.ping()
            logger.info("‚úÖ M√©triques Redis connect√©es")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis non disponible pour m√©triques: {e}")

        # Stockage en m√©moire pour moyenne glissante (fallback)
        self.processing_times = deque(maxlen=100)  # 100 derni√®res op√©rations
        self.total_processed = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.last_update = datetime.utcnow()

    def _get_key(self, metric_name: str) -> str:
        """G√©n√®re la cl√© Redis pour une m√©trique"""
        return f"news:metrics:{metric_name}"

    def record_processing_time(self, duration_ms: float) -> None:
        """
        Enregistre le temps de traitement d'une actualit√©

        Args:
            duration_ms: Dur√©e en millisecondes
        """
        # Stockage en m√©moire
        self.processing_times.append(duration_ms)

        # Stockage Redis (liste limit√©e √† 100)
        if self.redis_client:
            try:
                key = self._get_key("processing_times")
                self.redis_client.lpush(key, duration_ms)
                self.redis_client.ltrim(key, 0, 99)  # Garder seulement les 100 derniers
            except Exception as e:
                logger.error(f"‚ùå Erreur enregistrement temps: {e}")

    def get_avg_processing_time(self) -> float:
        """Calcule la moyenne glissante des temps de traitement"""
        if self.redis_client:
            try:
                key = self._get_key("processing_times")
                times = self.redis_client.lrange(key, 0, -1)
                if times:
                    times_float = [float(t) for t in times]
                    return sum(times_float) / len(times_float)
            except Exception as e:
                logger.error(f"‚ùå Erreur calcul moyenne Redis: {e}")

        # Fallback sur m√©moire locale
        if self.processing_times:
            return sum(self.processing_times) / len(self.processing_times)
        return 0.0

    def increment_total_processed(self) -> None:
        """Incr√©mente le compteur total d'actualit√©s trait√©es"""
        self.total_processed += 1

        if self.redis_client:
            try:
                key = self._get_key("total_processed")
                self.redis_client.incr(key)
            except Exception as e:
                logger.error(f"‚ùå Erreur incr√©mentation total: {e}")

    def increment_cache_hit(self) -> None:
        """Incr√©mente le compteur de cache hits"""
        self.cache_hits += 1

        if self.redis_client:
            try:
                key = self._get_key("cache_hits")
                self.redis_client.incr(key)
            except Exception as e:
                logger.error(f"‚ùå Erreur incr√©mentation cache_hits: {e}")

    def increment_cache_miss(self) -> None:
        """Incr√©mente le compteur de cache misses"""
        self.cache_misses += 1

        if self.redis_client:
            try:
                key = self._get_key("cache_misses")
                self.redis_client.incr(key)
            except Exception as e:
                logger.error(f"‚ùå Erreur incr√©mentation cache_misses: {e}")

    def increment_processed_today(self) -> None:
        """Incr√©mente le compteur journalier avec expiration automatique"""
        if self.redis_client:
            try:
                # Cl√© avec date du jour
                today = datetime.utcnow().strftime("%Y-%m-%d")
                key = self._get_key(f"processed_today:{today}")

                # Incr√©menter
                self.redis_client.incr(key)

                # D√©finir expiration √† minuit + 24h (pour garder l'historique d'hier)
                tomorrow = datetime.utcnow() + timedelta(days=1)
                midnight = tomorrow.replace(hour=0, minute=0, second=0, microsecond=0)
                ttl_seconds = int((midnight - datetime.utcnow()).total_seconds()) + 86400

                self.redis_client.expire(key, ttl_seconds)
            except Exception as e:
                logger.error(f"‚ùå Erreur incr√©mentation journali√®re: {e}")

    def get_processed_today(self) -> int:
        """R√©cup√®re le nombre d'actualit√©s trait√©es aujourd'hui"""
        if self.redis_client:
            try:
                today = datetime.utcnow().strftime("%Y-%m-%d")
                key = self._get_key(f"processed_today:{today}")
                count = self.redis_client.get(key)
                return int(count) if count else 0
            except Exception as e:
                logger.error(f"‚ùå Erreur r√©cup√©ration compteur journalier: {e}")
        return 0

    def get_total_processed(self) -> int:
        """R√©cup√®re le nombre total d'actualit√©s trait√©es"""
        if self.redis_client:
            try:
                key = self._get_key("total_processed")
                count = self.redis_client.get(key)
                return int(count) if count else self.total_processed
            except Exception as e:
                logger.error(f"‚ùå Erreur r√©cup√©ration total: {e}")
        return self.total_processed

    def get_cache_hits(self) -> int:
        """R√©cup√®re le nombre de cache hits"""
        if self.redis_client:
            try:
                key = self._get_key("cache_hits")
                count = self.redis_client.get(key)
                return int(count) if count else self.cache_hits
            except Exception as e:
                logger.error(f"‚ùå Erreur r√©cup√©ration cache_hits: {e}")
        return self.cache_hits

    def get_cache_misses(self) -> int:
        """R√©cup√®re le nombre de cache misses"""
        if self.redis_client:
            try:
                key = self._get_key("cache_misses")
                count = self.redis_client.get(key)
                return int(count) if count else self.cache_misses
            except Exception as e:
                logger.error(f"‚ùå Erreur r√©cup√©ration cache_misses: {e}")
        return self.cache_misses

    def set_last_update(self) -> None:
        """Enregistre l'horodatage de la derni√®re mise √† jour"""
        self.last_update = datetime.utcnow()

        if self.redis_client:
            try:
                key = self._get_key("last_update")
                self.redis_client.set(key, self.last_update.isoformat())
            except Exception as e:
                logger.error(f"‚ùå Erreur enregistrement last_update: {e}")

    def get_last_update(self) -> str:
        """R√©cup√®re l'horodatage de la derni√®re mise √† jour"""
        if self.redis_client:
            try:
                key = self._get_key("last_update")
                timestamp = self.redis_client.get(key)
                if timestamp:
                    return timestamp
            except Exception as e:
                logger.error(f"‚ùå Erreur r√©cup√©ration last_update: {e}")

        return self.last_update.isoformat()

    def get_all_stats(self) -> Dict[str, Any]:
        """
        R√©cup√®re toutes les statistiques en une seule fois

        Returns:
            {
                "total_news": 187,
                "processed_today": 12,
                "avg_embedding_time_ms": 640.5,
                "cache_hits": 18,
                "cache_misses": 2,
                "last_update": "2025-10-25T18:42:00Z"
            }
        """
        return {
            "total_processed": self.get_total_processed(),
            "processed_today": self.get_processed_today(),
            "avg_processing_time_ms": round(self.get_avg_processing_time(), 2),
            "cache_hits": self.get_cache_hits(),
            "cache_misses": self.get_cache_misses(),
            "last_update": self.get_last_update()
        }

    def reset_stats(self) -> None:
        """R√©initialise toutes les statistiques (admin uniquement)"""
        if self.redis_client:
            try:
                # Supprimer toutes les cl√©s de m√©triques
                pattern = "news:metrics:*"
                keys = self.redis_client.keys(pattern)
                if keys:
                    self.redis_client.delete(*keys)
                logger.info("üîÑ M√©triques r√©initialis√©es")
            except Exception as e:
                logger.error(f"‚ùå Erreur reset stats: {e}")

        # Reset m√©moire locale
        self.processing_times.clear()
        self.total_processed = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.last_update = datetime.utcnow()


# Instance globale
news_metrics = NewsMetrics()
